
# -*- coding: utf-8 -*-
"""DLassign2offlinedataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vaxxu5vx4OnCZADYX60uJ15HEwinD4mn
"""

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 1. Load dataset
df = pd.read_csv('IMDB Dataset.csv')

# 2. Convert labels: positive -> 1, negative -> 0
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

# 3. Extract text and labels
texts = df['review'].astype(str).tolist()
labels = df['sentiment'].astype(int).tolist()

# 4. Tokenize text
max_words = 10000
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)

# 5. Convert texts to binary matrix (bag of words model)
x_data = tokenizer.texts_to_matrix(texts, mode='binary')
y_data = np.array(labels).astype('float32')

# 6. Split into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=42)

# 7. Build model
model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(max_words,)))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 8. Compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 9. Train model
history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.2, verbose=1)

# 10. Evaluate
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# 11. Predict on test set
predictions = model.predict(x_test)

# 12. Show sample predictions
for i in range(10):
    predicted_label = 1 if predictions[i] >= 0.5 else 0
    print(f"Review {i+1}: Predicted = {predicted_label}, Actual = {int(y_test[i])}")













#netwala
from tensorflow.keras.datasets import imdb



(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words = 10000)



import numpy as np

def vectorize_sequences(sequences, dimensions = 10000):
  results = np.zeros((len(sequences), dimensions))
  for i,sequences in enumerate(sequences):
    results[i, sequences] = 1
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)






y_train = np.asarray(train_label).astype('float32')
y_test = np.asarray(test_label).astype('float32')






from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense








model = Sequential()
model.add(Dense(16, input_shape=(10000, ), activation = "relu"))
model.add(Dense(16, activation = "relu"))
model.add(Dense(1, activation = "sigmoid"))






model.compile(optimizer='adam', loss = 'mse', metrics = ['accuracy'])





model.summary()







history = model.fit(x_train, y_train, validation_split = 0.3, epochs = 20, verbose = 1, batch_size = 512)









# 1. Evaluate the model on test data
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# 2. Make predictions
predictions = model.predict(x_test)

# 3. Print predictions vs actual for first 10 samples
for i in range(10):
    predicted_label = 1 if predictions[i] >= 0.5 else 0
    print(f"Review {i+1}: Predicted = {predicted_label}, Actual = {int(y_test[i])}")



-------------------------------------------------------------------------------------------------------------------------



Theory for the Practical:
This code demonstrates the process of building and evaluating a neural network model to classify movie reviews (positive or negative) from the IMDB dataset using Keras and TensorFlow. Letâ€™s break down the steps involved:

1. Loading and Preprocessing the IMDB Dataset
python
Copy
Edit
from tensorflow.keras.datasets import imdb

(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words = 10000)
The IMDB dataset contains 50,000 movie reviews, where each review is classified as positive (1) or negative (0).

imdb.load_data() loads the dataset and limits the number of unique words to the top 10,000 most frequent words (num_words = 10000). The function returns two sets:

train_data: The review data (in the form of a list of integer-encoded words).

train_label: The labels (1 for positive, 0 for negative).

test_data: Review data for testing.

test_label: Labels for testing.

2. Vectorizing the Sequences
python
Copy
Edit
import numpy as np

def vectorize_sequences(sequences, dimensions = 10000):
  results = np.zeros((len(sequences), dimensions))
  for i, sequences in enumerate(sequences):
    results[i, sequences] = 1
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
Vectorization is a method to convert the sequence of word indices into a binary matrix where:

Each row corresponds to one review.

Each column represents whether a specific word (from the top 10,000 words) is present in the review or not.

For example, if a review contains the words corresponding to indices 1, 2, and 5, the vectorized form of the review would have 1s at indices 1, 2, and 5, and 0s elsewhere.

3. Converting Labels to Numpy Array
python
Copy
Edit
y_train = np.asarray(train_label).astype('float32')
y_test = np.asarray(test_label).astype('float32')
The labels (train_label and test_label) are converted to NumPy arrays and cast to float32 to match the expected format by the neural network (especially since binary classification outputs can be in floating-point format).

4. Building the Neural Network Model
python
Copy
Edit
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(16, input_shape=(10000, ), activation = "relu"))
model.add(Dense(16, activation = "relu"))
model.add(Dense(1, activation = "sigmoid"))
Model Architecture:

Sequential: A simple linear stack of layers.

Dense Layer 1: A fully connected layer with 16 neurons and ReLU (Rectified Linear Unit) activation. This layer takes a vector of length 10,000 (the number of words) as input.

Dense Layer 2: Another fully connected layer with 16 neurons and ReLU activation.

Output Layer: The output layer has 1 neuron and uses the sigmoid activation function, suitable for binary classification (positive or negative).

5. Compiling the Model
python
Copy
Edit
model.compile(optimizer='adam', loss = 'mse', metrics = ['accuracy'])
Optimizer: Adam is used, a popular optimization algorithm for training neural networks.

Loss Function: Mean Squared Error (MSE) is used here, though it's more typical for regression tasks. For binary classification, binary cross-entropy would be more appropriate, but MSE can still work in this case since it minimizes the squared error between 0 and 1.

Metrics: The model will track accuracy during training to monitor how well it performs on the task.

6. Training the Model
python
Copy
Edit
history = model.fit(x_train, y_train, validation_split = 0.3, epochs = 20, verbose = 1, batch_size = 512)
Training: The model is trained on the training data (x_train, y_train) for 20 epochs.

Validation Split: 30% of the training data is used as a validation set during training to monitor performance on unseen data.

Batch Size: The training process is divided into batches of 512 samples, which helps in efficient gradient computation.

Verbose: The training progress is shown for each epoch.

7. Evaluating the Model on Test Data
python
Copy
Edit
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"\nTest Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")
The model is evaluated on the test dataset (x_test, y_test).

The evaluation returns:

Test Loss: How well the model performs on the test data using the MSE loss function.

Test Accuracy: The proportion of correct predictions on the test data.

8. Making Predictions
python
Copy
Edit
predictions = model.predict(x_test)
Prediction: The model is used to predict the sentiment (positive or negative) for each review in the test set.

The predict() method outputs a value between 0 and 1 for each review. A value closer to 1 represents a positive review, while a value closer to 0 represents a negative review.

9. Printing Predictions vs. Actual Values
python
Copy
Edit
for i in range(10):
    predicted_label = 1 if predictions[i] >= 0.5 else 0
    print(f"Review {i+1}: Predicted = {predicted_label}, Actual = {int(y_test[i])}")
For the first 10 test samples:

The predicted label is determined based on whether the predicted probability is greater than or equal to 0.5. If it is, the review is classified as positive (1), otherwise as negative (0).

The predicted label is compared with the actual label from y_test to check the performance of the model on individual reviews.

Key Takeaways:
Data Preprocessing: The text data is converted into binary vectors where each word's presence is represented by a 1 in the corresponding index position.

Model Architecture: A simple feedforward neural network is used with two hidden layers and a sigmoid output layer for binary classification.

Evaluation: The model is evaluated on the test set using accuracy and loss as performance metrics.

Binary Classification: The model outputs a probability between 0 and 1 for each review, which is converted into a binary class (positive or negative).

Predictions: Predictions are made for the test samples, and the predicted labels are compared to the actual labels to see how well the model is performing.
